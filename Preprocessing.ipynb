{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efa66986",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center>\n",
    "\n",
    "<h1><b>Fraud Detection Project</b></h1>\n",
    "<h3>Universidad Carlos III de Madrid · Bluetab</h3>\n",
    "\n",
    "<p><em>Development of a predictive system using Machine Learning to identify fraudulent transactions and strengthen financial security.</em></p>\n",
    "\n",
    "</center>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67068ffb",
   "metadata": {},
   "source": [
    "Identify types of variables (numerical or categorical) because SMOTE cannot deal with categorical ones and SMOTENC needs to know which are them to use a special metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5202eb9e",
   "metadata": {},
   "source": [
    "### Notebook Overview\n",
    "\n",
    "This notebook is the second part of the *Bluetab–UC3M Fraud Detection Project*.  \n",
    "Its main goal is to perform adequate feature engineering and address the class imbalance within the merged dataset to provide an insightful and robust dataset for the machine learning models to be applied.  \n",
    "\n",
    "Throughout this notebook, we:\n",
    "- Address the class imbalance through various approaches (SMOTE, no SMOTE)\n",
    "- Perform feature engineering\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962452f",
   "metadata": {},
   "source": [
    "# **4. Data Preprocessing & Class Balancing**\n",
    "\n",
    "In this section, we start the preprocessing stage of the project. After importing the merged dataset, we will focus on addressing the strong class imbalance, identified during the EDA, using SMOTE (Synthetic Minority Oversampling Technique). Then we apply feature engineering to extract more information from out available variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d56b0e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import chardet\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import polars as pl\n",
    "import os\n",
    "from IPython.display import display\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from itertools import combinations\n",
    "from pytz import timezone\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pvlib.location import Location\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25c3651",
   "metadata": {},
   "source": [
    "## Prepare training and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fb7471e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (286068, 45)\n",
      "Shape of y: (286068,)\n"
     ]
    }
   ],
   "source": [
    "# Upload merged dataset from EDA.ipynb\n",
    "df = pd.read_csv('content/merged.csv')\n",
    "\n",
    "# Separate Features (X) and Target (y)\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of y:\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331c8eb",
   "metadata": {},
   "source": [
    "Identify types of variables (numerical or categorical) because SMOTE cannot deal with categorical ones and SMOTENC needs to know which are them to use a special metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9822d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate between numerical and categorical features\n",
    "cat_cols = list(X.select_dtypes(include=['object']).columns)\n",
    "num_cols = list(X.select_dtypes(include=['float64', 'int64']).columns)\n",
    "\n",
    "print(cat_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae307a29",
   "metadata": {},
   "source": [
    "We removed 'transaction_id', 'customer_id', 'device_id', 'ip_address', 'email', 'phone' and 'name' as they are unique identifiers and are noise for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6724b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed id columns\n",
    "id_cols = [\n",
    "    'transaction_id', 'customer_id', 'device_id',\n",
    "    'ip_address', 'email', 'phone', 'name'\n",
    "]\n",
    "X = X.drop(columns=id_cols, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa5f47b",
   "metadata": {},
   "source": [
    "Transform join_date in an useful attribute compatible with the models, days since the client registered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095ccaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['join_date'] = pd.to_datetime(X['join_date'], errors='coerce')                    # Convert to datetime\n",
    "X['customer_days_since_join'] = (X['join_date'].max() - X['join_date']).dt.days     # Calculate days since join\n",
    "X = X.drop(columns=['join_date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbcf680",
   "metadata": {},
   "source": [
    "Reduce cardinality in the categories by collapsing the most rare ones into \"other\". This threshold is established with 'top'. If we do not deal with this, SMOTENC will have memory problems as it cannot deal with so many levels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2acbb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "top= 20\n",
    "def rare_values(series, top=top):\n",
    "    top_vals = series.value_counts().nlargest(top).index        # Get the top N most frequent values\n",
    "    return series.where(series.isin(top_vals), 'Other')         # Replace rare values with 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe3470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['merchant_country', 'customer_country', 'city', 'merchant', 'zip_code']: # TODO hacerlo automatico en vez de pasando la lista de variables\n",
    "    if X[col].nunique() > top:                                          # Only apply if there are more than 'top' unique values\n",
    "        X[col] = rare_values(X[col], top=top)                           # Replace rare values\n",
    "        print(f\"Reduced categories in {col} to top {top} + 'Other'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical cardinality after bucketing (BEFORE encoding)\n",
    "\n",
    "def plot_categorical_cardinality(dfX, title=\"Categorical Cardinality (after bucketing)\", top_used=20):\n",
    "    # detect categories\n",
    "    cat_cols = list(dfX.select_dtypes(include='object').columns)\n",
    "    if not cat_cols:\n",
    "        print(\"No remaining object (categorical) columns. Did you encode already?\")\n",
    "        return\n",
    "    \n",
    "    # cardinality and order\n",
    "    card = (dfX[cat_cols]\n",
    "            .nunique(dropna=False)                # if there is Unknown/NaN, count it as a category\n",
    "            .sort_values(ascending=True)\n",
    "            .to_frame('n_categories'))\n",
    "    \n",
    "    # plot\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    h = max(3, 0.40 * len(card))                  \n",
    "    fig, ax = plt.subplots(figsize=(9, h))\n",
    "    \n",
    "    sns.barplot(\n",
    "        data=card.reset_index(),\n",
    "        x='n_categories', y='index',\n",
    "        orient='h', ax=ax, palette=\"viridis\"\n",
    "    )\n",
    "    \n",
    "    # titles and labels\n",
    "    ax.set_title(f\"{title}\\n(Top-{top_used} rare bucketing applied)\", fontsize=13, weight=\"bold\")\n",
    "    ax.set_xlabel(\"Number of distinct categories\")\n",
    "    ax.set_ylabel(\"Feature\")\n",
    "    \n",
    "    # number of categories on the bars\n",
    "    for i, (feat, ncat) in enumerate(card['n_categories'].items()):\n",
    "        ax.text(ncat + max(card['n_categories']) * 0.01, i, f\"{int(ncat)}\",\n",
    "                va=\"center\", ha=\"left\", fontsize=10, weight=\"bold\", color=\"#2f2f2f\")\n",
    "    \n",
    "    sns.despine(ax=ax, top=True, right=True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # also display the cardinality table\n",
    "    display(card.sort_values('n_categories', ascending=False))\n",
    "\n",
    "plot_categorical_cardinality(X, title=\"Categorical Cardinality (after Top-N + 'Other')\", top_used=top)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b769dd",
   "metadata": {},
   "source": [
    "## SMOTE\n",
    "\n",
    "SMOTENC balances datasers with both numerical and categorical variables. For **numerical features** it interpolates values between a sample and its neighbors (knn) and for **categorical features** it assigns the most frequent category among neighbors. This way, new synthetic minority samples are generated following the variables' distribution, without losing the mixed data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8290e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "knn = 3     # Number of nearest neighbors for SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a2f95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate the categorical columns and their indices after dropping columns\n",
    "cat_cols = list(X.select_dtypes(include='object').columns)\n",
    "cat_idx = [X.columns.get_loc(c) for c in cat_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e2121",
   "metadata": {},
   "source": [
    "Codify the categories to integers for SMOTENC without real order, allowing -1 if a new category in the test appears to avoid errors. Apply SMOTENC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcf14e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codify categories to integers                                      # TODO meterlo en un transformer para uqe lo haga directo/hacer una funcion, a lo mejor usar one hot para alguna de las columans\n",
    "encode = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)   # Handle unknown categories in test set\n",
    "X[cat_cols] = encode.fit_transform(X[cat_cols])                                 # Fit and transform the categorical columns\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTENC to the training data\n",
    "print(\"Before SMOTENC:\", Counter(y_train))\n",
    "\n",
    "smotenc = SMOTENC(categorical_features=cat_idx, random_state=42, k_neighbors=knn) \n",
    "X_train_res, y_train_res = smotenc.fit_resample(X_train, y_train)\n",
    "\n",
    "print(\"After SMOTENC:\", Counter(y_train_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10eee105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class balance before/after (train) \n",
    "import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "def _plot_counts_with_pct(ax, s, title):\n",
    "    counts = s.value_counts().sort_index()\n",
    "    total = counts.sum()\n",
    "    bars = sns.barplot(x=counts.index.astype(str), y=counts.values, ax=ax, palette=\"viridis\")\n",
    "    ax.set_title(title, fontsize=12, weight=\"bold\")\n",
    "    ax.set_xlabel(\"Class\"); ax.set_ylabel(\"Count\")\n",
    "    for i, v in enumerate(counts.values):\n",
    "        pct = 100.0 * v / total\n",
    "        ax.text(i, v, f\"{v:,}\\n({pct:.1f}%)\", ha=\"center\", va=\"bottom\", fontsize=10, weight=\"bold\")\n",
    "    sns.despine(ax=ax)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(11, 4.5))\n",
    "_plot_counts_with_pct(ax[0], y_train, \"Before SMOTENC (Train)\")\n",
    "_plot_counts_with_pct(ax[1], y_train_res, \"After SMOTENC (Train)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440fb58",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "The purpose of feature engineering is to clean and transform our current variables into more usable and predictable parameters of our machine learning models.\n",
    "\n",
    "From Section 1 there are several proposed features that can be implemented into the merged dataset by simply grouping differently.\n",
    "\n",
    "- country_transaction_count and city_transaction_count\n",
    "\n",
    "- avg_city_transactions_per_country\n",
    "\n",
    "- merchant_transaction_count and merchant_avg_amount\n",
    "\n",
    "- customer_transaction_count and customer_avg_amount\n",
    "\n",
    "- ip_per_customer and ip_per_device\n",
    "\n",
    "- customer_age_group\n",
    "\n",
    "- The variable join_date can be transformed into join_year and customer_tenure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec2018d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing with a subset (first 1000 rows)\n",
    "df = df.iloc[:1000, :].copy()\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb5aaa1",
   "metadata": {},
   "source": [
    "Applying the feature engineering with pandas groupby and transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d9994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the transaction count for each customer country\n",
    "df['country_transaction_count'] = (\n",
    "    df.groupby('customer_country')['transaction_id'].transform('count')\n",
    ")\n",
    "\n",
    "# Calculate the average number of transactions per city within each customer country\n",
    "# First, calculate transaction count per city\n",
    "df['city_transaction_count'] = (\n",
    "    df.groupby(['customer_country', 'city'])['transaction_id'].transform('count')\n",
    ")\n",
    "\n",
    "# Then, calculate the average of these counts within each country\n",
    "df['avg_city_transactions_per_country'] = (\n",
    "    df.groupby('customer_country')['city_transaction_count'].transform('mean')\n",
    ")\n",
    "\n",
    "# Calculate the transaction count for each merchant\n",
    "df['merchant_transaction_count'] = (\n",
    "    df.groupby('merchant')['transaction_id'].transform('count')\n",
    ")\n",
    "\n",
    "# Calculate the average transaction amount for each merchant\n",
    "avg_amount_per_merchant = (\n",
    "    df.groupby('merchant')['Amount'].mean().reset_index(name='avg_amount')\n",
    ")\n",
    "\n",
    "# Calculate the transaction count for each customer\n",
    "txn_count_per_customer = (\n",
    "    df.groupby('customer_id')['transaction_id'].count().reset_index(name='transaction_count')\n",
    ")\n",
    "\n",
    "# Calculate the average transaction amount for each customer\n",
    "avg_amount_per_customer = (\n",
    "    df.groupby('customer_id')['Amount'].mean().reset_index(name='avg_amount')\n",
    ")\n",
    "\n",
    "# Calculate the IP addresses for each customer\n",
    "ip_per_customer = (\n",
    "    df.groupby('customer_id')['ip_address'].unique().reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the IP addresses for each device\n",
    "ip_per_device = (\n",
    "    df.groupby('device_id')['ip_address'].unique().reset_index()\n",
    ")\n",
    "\n",
    "# Separate the customer age into different brackets\n",
    "bins = [0, 25, 35, 45, 60, 100]\n",
    "labels = ['<25', '25-35', '35-45', '45-60', '60+']\n",
    "df['age_bracket'] = pd.cut(df['age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Transform customer join_date into join_year and customer_tenure\n",
    "df['join_date'] = pd.to_datetime(df['join_date'], errors='coerce')\n",
    "df['join_year'] = df['join_date'].dt.year\n",
    "df['customer_tenure'] = (pd.Timestamp('now') - df['join_date']).dt.days // 365"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
