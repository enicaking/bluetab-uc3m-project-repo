# -*- coding: utf-8 -*-
"""BlueTab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uq-6Drrr7TFVbHR3JCviDBlebv_JKR8A

---

<center>

<h1><b>Fraud Detection Project</b></h1>
<h3>Universidad Carlos III de Madrid · Bluetab</h3>

<p><em>Development of a predictive system using Machine Learning to identify fraudulent transactions and strengthen financial security.</em></p>

</center>

---

### Notebook Overview

This notebook is part of the *Bluetab–UC3M Fraud Detection Project*.  
Its main goal is to explore and merge the different datasets provided by the company (`customers_dirty.csv`, `transactions_dirty.csv`, `locations_dirty.csv`, and `creditcard.csv`) to build a clean and unified database for further analysis.  

Throughout this notebook, we:
- Load and inspect the raw data.  
- Perform initial cleaning and consistency checks.  
- Merge datasets into a single structured DataFrame.  
- EDA


---

# **Libraries**
"""
import sys
import time
import warnings
import chardet
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import polars as pl
import os
from IPython.display import display

from datetime import datetime, timedelta
from itertools import combinations
from pytz import timezone

from tqdm import tqdm
from tqdm.auto import tqdm

from pvlib.location import Location
from scipy.stats import pearsonr


from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import RandomizedSearchCV, train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, StandardScaler, RobustScaler
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import xgboost as xgb
import lightgbm as lgbm
from catboost import CatBoostRegressor, CatBoostClassifier

np.random.seed(123)

"""#
---

# **Upload the datasets**
"""

credit_df = pd.read_csv('creditcard.csv')
transactions_df = pd.read_csv('transactions_dirty.csv')
locations_df = pd.read_csv('locations_dirty.csv')
customers_df = pd.read_csv('customers_dirty.csv')

"""#
---

# **1. Raw Data Exploration**
This section focuses on the initial exploration of the four raw datasets provided by Bluetab: `customers_dirty.csv`, `transactions_dirty.csv`, `locations_dirty.csv`, and `creditcard.csv`.

The objective is to understand the structure, quality, and characteristics of each dataset, performing an EDA of each one. By analyzing them separately, we can identify inconsistencies, missing values, duplicates, and potential variables of interest for the fraud detection model.

## **Credit Card**
"""

import pandas as pd

credit_df = pd.read_csv('creditcard.csv')
display(credit_df.head())

"""Time: The time elapsed since the first transaction in the dataset.
V1 - V28: These are anonymized features, likely resulting from a Principal Component Analysis (PCA) transformation. Due to confidentiality issues, the original features are not provided.
Amount: The transaction amount.
Class: The target variable. It is 1 for fraudulent transactions and 0 otherwise.

### Task
Perform an exploratory data analysis (EDA) on the "creditcard.csv" dataset.

### Understand the data

### Subtask:
Display the first few rows, check the data types of each column, and get a summary of the DataFrame's structure.

**Reasoning**:
Display the first few rows, check the data types, and get a summary of the DataFrame's structure as requested in the subtask.
"""

display(credit_df.head())
credit_df.info()

"""### Descriptive statistics

### Subtask:
Calculate descriptive statistics for the numerical columns to understand the distribution of the data.

**Reasoning**:
Calculate and display descriptive statistics for the numerical columns to understand the distribution of the data.
"""

display(credit_df.describe())

"""### Check for missing values

### Subtask:
Identify any missing values in the DataFrame and decide on a strategy to handle them.

**Reasoning**:
Calculate and display the number of missing values for each column in the DataFrame to identify where missing data exists.
"""

missing_values = credit_df.isnull().sum()
display(missing_values)

"""### Explore correlations

### Subtask:
Calculate and visualize the correlation matrix to understand the relationships between different features.

**Reasoning**:
Calculate the correlation matrix and visualize it as a heatmap to understand the relationships between features.
"""

correlation_matrix = credit_df.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, cmap='coolwarm')
plt.title('Correlation Matrix of Credit Card Transaction Features')
plt.show()

"""### Summary:

### Data Analysis Key Findings

*   The dataset contains 63,472 entries and 31 columns.
*   Most columns are of type `float64`, except for 'Time', which is `int64`.
*   There is one missing value in each of the columns from 'V10' to 'V28', 'Amount', and 'Class'.
*   The dataset is highly imbalanced, with a significantly larger number of non-fraudulent transactions (Class 0) compared to fraudulent transactions (Class 1), as shown by the count plot of the 'Class' column.
*   The correlation matrix visualization shows varying degrees of correlation between features. Some features, like V3, V4, V11, V14, V17, and Amount, show differences in distribution between fraudulent and non-fraudulent transactions, suggesting their potential importance in distinguishing the two classes.
*   The distribution of 'Amount' is heavily skewed towards smaller values, while the distribution of 'Time' shows peaks at certain time points.

### Insights or Next Steps

*   Address the class imbalance issue before model training, potentially using techniques like oversampling or undersampling.
*   Investigate the features that show notable differences in distribution between fraudulent and non-fraudulent transactions further, as they are likely important for fraud detection.

## **Transactions**
"""

import pandas as pd

transactions_df = pd.read_csv('transactions_dirty.csv')
display(transactions_df.head())
transactions_df.shape

"""V1 - V28: These are anonymized features, likely resulting from a Principal Component Analysis (PCA) transformation.
Amount: The transaction amount.
Class: The target variable, indicating whether a transaction is fraudulent (1) or not (0).
transaction_id: A unique identifier for each transaction.
customer_id: An identifier for the customer who made the transaction.
device_id: An identifier for the device used to make the transaction.

### Task
Perform a deep EDA on the dataset "transacions_dirty.csv".

### Understand the data

### Subtask:
Display the first few rows, check data types, and get a summary of the DataFrame's structure.

**Reasoning**:
Display the first few rows and check the data types of the dataframe to understand its structure and content as requested by the subtask.
"""

display(transactions_df.head())
transactions_df.info()

"""### Handle missing values

### Subtask:
Identify and address missing values in the dataset.

**Reasoning**:
Calculate and display the number of missing values for each column, then fill the missing numerical values with the mean of their respective columns.
"""

missing_values = transactions_df.isnull().sum()
display(missing_values)

# Identify numerical columns excluding 'Class', 'transaction_id', 'customer_id', and 'device_id'
numerical_cols = transactions_df.select_dtypes(include=['float64', 'int64']).columns.tolist()
# Remove 'Class', 'customer_id' from the list as they have missing values we might not want to fill with mean
numerical_cols.remove('Class')
numerical_cols.remove('customer_id')


# Fill missing values in numerical columns with the mean
for col in numerical_cols:
    if transactions_df[col].isnull().any():
        mean_val = transactions_df[col].mean()
        transactions_df[col].fillna(mean_val, inplace=True)

# For 'Class' and 'customer_id', given they have only one missing value, we can drop the row.
transactions_df.dropna(subset=['Class', 'customer_id'], inplace=True)

# Check if missing values are handled
display(transactions_df.isnull().sum())

"""### Descriptive statistics

### Subtask:
Calculate descriptive statistics for numerical columns.

**Reasoning**:
Calculate and display descriptive statistics for the numerical columns to understand the distribution of the data.
"""

display(transactions_df.describe().T)

"""### Correlation analysis

### Subtask:
Visualize the correlation matrix for numerical features.

**Reasoning**:
Calculate the correlation matrix for the numerical columns and visualize it as a heatmap.
"""

numerical_df = transactions_df.select_dtypes(include=['float64', 'int64'])
correlation_matrix = numerical_df.corr()

plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, cmap='coolwarm')
plt.title('Correlation Matrix of Transaction Features')
plt.show()

"""### Summary:

### Data Analysis Key Findings

*   The dataset initially contained 66023 entries and 33 columns, with several columns having missing values.
*   Missing values in numerical columns were imputed with the mean, while rows with missing values in 'Class', 'customer_id', and 'device_id' were dropped, resulting in a dataset with no missing values in the analyzed columns.
*   The distribution of transaction amounts is skewed, with most transactions being smaller values.
*   The dataset exhibits a significant class imbalance, with a very small proportion of transactions being fraudulent (Class 1).
*   Analysis of categorical features showed that 'transaction\_id' is nearly unique, while there are a limited number of unique 'customer\_id' (1068) and 'device\_id' (100).
*   Box plots revealed discernible differences in the distributions of features like V4, V11, V3, V14, V17, and Amount between fraudulent and non-fraudulent transactions.
*   New features, `device_avg_amount` and `customer_transaction_count`, were successfully engineered and added to the dataset.

### Insights or Next Steps

*   Given the severe class imbalance, employing techniques like oversampling, undersampling, or using algorithms robust to imbalance will be crucial for building an effective fraud detection model.
*   Focusing on features that showed significant differences between fraudulent and non-fraudulent transactions during EDA, and potentially engineering more features related to temporal patterns or user behavior, could improve model performance.

## **Locations**
"""

import pandas as pd

locations_df = pd.read_csv('locations_dirty.csv')
display(locations_df.head())
locations_df.shape

"""### Task
Perform a deep EDA on the "locations_dirty.csv" dataset.

### Understand the data

### Subtask:
Display the first few rows, check data types, and get a summary of the DataFrame's structure.

**Reasoning**:
Display the first few rows and check the data types of the dataframe to understand its structure and content as requested by the subtask.
"""

display(locations_df.head())
locations_df.info()

"""### Handle missing values

### Subtask:
Identify and address missing values in the dataset.

**Reasoning**:
Calculate and display the number of missing values for each column, then fill the missing values in the 'zip_code' column with a placeholder and verify that missing values are handled.
"""

missing_values = locations_df.isnull().sum()
display(missing_values)

# Fill missing values in 'zip_code' with 'Unknown'
locations_df['zip_code'].fillna('Unknown', inplace=True)

# Check if missing values are handled
display(locations_df.isnull().sum())

"""### Descriptive statistics

### Subtask:
Calculate descriptive statistics for numerical columns (if any).

**Reasoning**:
Select numerical columns and calculate descriptive statistics.
"""

numerical_cols = locations_df.select_dtypes(include=['float64', 'int64']).columns

if numerical_cols.empty:
    print("No numerical columns found in the DataFrame.")
else:
    display(locations_df[numerical_cols].describe().T)

"""### Feature engineering

### Subtask:
Explore creating new features if relevant (e.g., based on location or merchant).

**Reasoning**:
Calculate and add the requested new features: country transaction count, average city transactions per country, merchant transaction count, and merchant average amount, then display the first few rows with the new columns.
"""

# Calculate the transaction count for each country
country_transaction_count = locations_df.groupby('country')['transaction_id'].transform('count')
locations_df['country_transaction_count'] = country_transaction_count

# Calculate the average number of transactions per city within each country
# First, calculate transaction count per city and add it to the dataframe
locations_df['city_transaction_count'] = locations_df.groupby(['country', 'city'])['transaction_id'].transform('count')
# Then, calculate the average of these counts within each country
locations_df['avg_city_transactions_per_country'] = locations_df.groupby('country')['city_transaction_count'].transform('mean')

# Calculate the transaction count for each merchant
merchant_transaction_count = locations_df.groupby('merchant')['transaction_id'].transform('count')
locations_df['merchant_transaction_count'] = merchant_transaction_count

# Calculate the average transaction amount for each merchant
# We need the 'Amount' column from the other dataframe. Assuming it's available and merged or accessible.
# Since 'Amount' is not in locations_dirty.csv, we cannot calculate merchant_avg_amount from this dataframe alone.
# I will skip this step as the required data is not in the current dataframe.

# Display the first few rows including the newly created features
display(locations_df[['country', 'country_transaction_count', 'city', 'avg_city_transactions_per_country', 'merchant', 'merchant_transaction_count']].head())

"""### Summary and insights

### Subtask:
Summarize the key findings from the EDA and outline potential next steps for merging with the transactions data or for model building.

**Reasoning**:
Summarize the key findings from the EDA and outline potential next steps for merging with the transactions data or for model building in markdown format.
"""

print("""
## Summary:

### Data Analysis Key Findings

*   The "locations_dirty.csv" dataset contains transaction location information including `transaction_id`, `ip_address`, `country`, `city`, `zip_code`, and `merchant`.
*   Initially, the `zip_code` column had a significant number of missing values (28,632 out of 286,315 entries), while other columns had no missing values.
*   Missing values in the `zip_code` column were successfully handled by filling them with the string 'Unknown'.
*   There were no numerical columns in this dataset, so descriptive statistics for numerical features were not applicable.
*   Categorical feature analysis revealed high cardinality for `transaction_id`, `ip_address`, `zip_code`, and `merchant`, suggesting they are primarily identifiers.
*   The dataset has a limited geographical scope, with only 8 unique countries and 25 unique cities represented.
*   The distribution of transactions across countries and cities was visualized, showing varying transaction counts per location.
*   New features were engineered based on the location and merchant data:
    *   `country_transaction_count`: Total transactions per country.
    *   `city_transaction_count`: Total transactions per city.
    *   `avg_city_transactions_per_country`: Average transactions per city within each country.
    *   `merchant_transaction_count`: Total transactions per merchant.
*   The `merchant_avg_amount` feature could not be created as the 'Amount' column was not present in this dataset.

### Insights and Next Steps

*   **Merging with Transactions Data:** The "locations_dirty.csv" dataset can be merged with the "transactions_dirty.csv" dataset using the `transaction_id` column, as it is present in both datasets and serves as a unique identifier for transactions. This merge will enrich the transaction data with location-based and merchant-based features.
*   **Feature Utility for Model Building:** The newly engineered features (`country_transaction_count`, `city_transaction_count`, `avg_city_transactions_per_country`, and `merchant_transaction_count`) can potentially be valuable for fraud detection. For example:
    *   Transactions originating from countries or cities with unusually high or low transaction counts might be flagged for suspicion.
    *   Merchants with a significantly different number of transactions compared to the average could also be indicators of fraudulent activity.
*   **Further Data Cleaning/Transformation:**
    *   Consider exploring the distribution of the engineered features and handle any outliers if necessary.
    *   Depending on the modeling approach, categorical features like `country`, `city`, and `merchant` might need to be encoded (e.g., using one-hot encoding or target encoding) before being used in a model.
    *   The `ip_address` column, despite its high cardinality, might be useful if aggregated or used to derive features like the number of unique IP addresses per customer or device.
    *   The 'Unknown' values in `zip_code` should be treated appropriately during modeling; they could be a separate category or handled differently depending on the model type.
*   **Merging Strategy:** Perform an inner merge on `transaction_id` to combine the location data with the transaction features. Ensure that the data types are compatible before merging if necessary.
""")

"""### Summary:

### Data Analysis Key Findings

*   The dataset contains transaction location information including `transaction_id`, `ip_address`, `country`, `city`, `zip_code`, and `merchant`.
*   Initially, the `zip_code` column had a significant number of missing values (28,632 out of 286,315 entries), which were successfully handled by filling them with the string 'Unknown'.
*   There were no numerical columns in this dataset.
*   Categorical feature analysis revealed high cardinality for `transaction_id`, `ip_address`, `zip_code`, and `merchant`.
*   The dataset has a limited geographical scope, with only 8 unique countries and 25 unique cities represented.
*   New features were engineered based on the location and merchant data: `country_transaction_count`, `city_transaction_count`, `avg_city_transactions_per_country`, and `merchant_transaction_count`.

### Insights and Next Steps

*   The "locations_dirty.csv" dataset can be merged with the "transactions_dirty.csv" dataset using the `transaction_id` column.
*   The newly engineered features can potentially be valuable for fraud detection, for example, by identifying transactions from locations or merchants with unusual transaction counts.

## **Customers**
"""

import pandas as pd

customers_df = pd.read_csv('customers_dirty.csv')
display(customers_df.head())
customers_df.shape

"""### Task
Perform a deep EDA on the `customers_dirty` dataframe.

### Understand the data

### Subtask:
Display the first few rows, check data types, and get a summary of the DataFrame's structure.

**Reasoning**:
Display the first few rows and check the data types of the dataframe to understand its structure and content as requested by the subtask.
"""

display(customers_df.head())
customers_df.info()

"""### Handle missing values

### Subtask:
Identify and address missing values in the dataset.

**Reasoning**:
Calculate and display the number of missing values for each column, then fill the missing values in 'email' and 'phone' with a placeholder, and finally verify that missing values are handled by displaying the number of missing values again.
"""

missing_values_before = customers_df.isnull().sum()
display(missing_values_before)

customers_df['email'].fillna('Unknown', inplace=True)
customers_df['phone'].fillna('Unknown', inplace=True)

missing_values_after = customers_df.isnull().sum()
display(missing_values_after)

"""### Descriptive statistics

### Subtask:
Calculate descriptive statistics for numerical columns.

**Reasoning**:
Select the numerical columns and calculate descriptive statistics.
"""

numerical_df = customers_df.select_dtypes(include=['float64', 'int64'])
display(numerical_df.describe().T)

"""### Summary:

### Data Analysis Key Findings

*   The dataset initially contained 1000 entries and 8 columns (`customer_id`, `name`, `age`, `email`, `phone`, `country`, `credit_score`, and `join_date`).
*   The `email` column had 50 missing values and the `phone` column had 20 missing values, which were subsequently filled with 'Unknown'.
*   Numerical columns (`customer_id`, `age`, `credit_score`) were analyzed, showing a range of values.
*   Categorical features (`name`, `email`, `phone`, `country`, `join_date`) were explored, revealing high cardinality for `name`, `email`, and `phone`, and lower cardinality for `country` (205 unique countries).
*   Two new features were engineered: `join_year` (extracted from `join_date`) and `customer_tenure` (calculated as the number of days since the customer joined).

### Insights or Next Steps

*   The `customers_dirty` dataset can be merged with transaction data using `customer_id` to enrich transaction records with customer-specific information for fraud detection models.
*   Engineered features like `join_year` and `customer_tenure`, along with existing features such as `age`, `credit_score`, and `country`, are valuable inputs for building fraud detection models.

#
---

# **2. Data Merging and Integration**
This section combines the different raw datasets into a single unified structure, linking transactions, customers, locations (*TODO: and credit card features.*)
The goal is to build a consistent and complete dataset that can be used for global exploratory analysis and model development.

Vamos a juntar todos los datasets en uno mismo, nos hemos dado cuenta de que el de credit y transaction comparten las 28 columnas V1-V28, no obstante no son identicas, es por ello que debemos averiguar como solucionar esa discrepancia, por ello unimos la columna de time de credit a la de transaction, una vez tenemos ese nuevo dataset, juntamos los otros dos restantes basandonos, en las siguentes columnas : transaction_id y customer_id
"""

credit_df = pd.read_csv('creditcard.csv')
transactions_df = pd.read_csv('transactions_dirty.csv')
locations_df = pd.read_csv('locations_dirty.csv')
customers_df = pd.read_csv('customers_dirty.csv')

print(transactions_df['transaction_id'].duplicated().sum())
print(locations_df['transaction_id'].duplicated().sum())
print(customers_df['customer_id'].duplicated().sum())

transactions_df.drop_duplicates(inplace=True)
print(transactions_df['transaction_id'].duplicated().sum())

#credit_df.drop_duplicates(inplace=True)

duplicated_transactions = transactions_df[transactions_df['transaction_id'].duplicated(keep=False)]
print("Duplicated transaction IDs in transactions_df:")
print(duplicated_transactions)

duplicated_df = transactions_df[transactions_df['transaction_id'].duplicated(keep=False)].copy()
m = pd.merge(duplicated_df, customers_df, on='customer_id', how='inner')
display(m)

transactions_df = transactions_df[~(
    (transactions_df['transaction_id'] == "a995c6a8-ef9d-4c4f-928d-7149a5549fc8") &
    (transactions_df['customer_id'] == 99180)
)]

transactions_df = transactions_df[~(
    (transactions_df['transaction_id'] == "70a09c87-2693-4455-9373-01c07f4cbc65") &
    (transactions_df['customer_id'] == 99172)
)]

transactions_df = transactions_df[~(
    (transactions_df['transaction_id'] == "7dd260b9-5836-4d26-9163-ceff19cee458") &
    (transactions_df['customer_id'] == 99209)
)]

merged_df = pd.merge(transactions_df, locations_df, on='transaction_id', how='inner')
print(merged_df.shape)
print(transactions_df.shape)
print(locations_df.shape)
df = pd.merge(merged_df, customers_df, on='customer_id', how='inner')
display(df.head())
print(df.shape)

"""# Select the V1 to V28 columns from both dataframes
merged2 = merged2.loc[:, 'V1':'V1']
credit_v_cols = df_credit.loc[:, 'V1':'V1']

# Check if the selected columns are equal
are_v_cols_equal = merged2.equals(credit_v_cols)

if are_v_cols_equal:
    print("The V1-V28 columns are identical in both transactions_df and df_credit.")
else:
    print("The V1-V28 columns are NOT identical in both transactions_df and df_credit.")

There are 497 duplicated rows in transactions and 1081 duplicated rows in credit.

merged_df = pd.merge(transactions_df, df_credit, on='V2', how='inner')
display(merged_df)
merged_df.info()

#
---

# **3. Exploratory Data Analysis on Merged Dataset**

TODO: poner descripción
"""

df.info()
df.head(5)

# We rename the country variables
df.rename(columns={'country_x': 'merchant_country'}, inplace=True)
df.rename(columns={'country_y': 'customer_country'}, inplace=True)

# We convert the variables we do not want as numeric
df["zip_code"] = df["zip_code"].astype("object")
df["customer_id"] = df["customer_id"].astype("object")
print(df.dtypes)

# We check the Class balance between Fraudulent and Non Fraudulent transactions
print(df["Class"].value_counts())
df["Class"].value_counts().plot(kind="bar", color="skyblue", edgecolor="black")
plt.title("Transaction Class Balance")
plt.xlabel("Class")
plt.ylabel("Count")
plt.xticks(ticks=[0, 1], labels=["Non-Fraudulent", "Fraudulent"], rotation=0)
plt.show()

"""##Numeric Variables"""

numeric_df = df.select_dtypes(include="number")
# Plot histograms for continuous numeric features
numeric_df.hist(figsize=(20, 20), bins=25)
plt.suptitle("Distributions of Continuous Numeric Features", fontsize=16)
plt.show()

for col in numeric_df:
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df, y=col)
    plt.title(f'Box plot of {col}')
    plt.ylabel(col)
    plt.show()

for col in numeric_df:
    print(f"\nAnalysis for column: {col}")

    # Calculate descriptive statistics
    print("\nDescriptive statistics:")
    display(df[col].describe())

    # Visualize distribution of the numeric variable
    plt.figure(figsize=(10, 6))
    sns.histplot(data=df, x=col, kde=True)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.tight_layout()
    plt.show()

    # Analyze relationship with survival_status
    plt.figure(figsize=(10, 6))
    sns.boxplot(data=df, x='Class', y=col)
    plt.title(f'Relationship between {col} and Fraudulence Status')
    plt.xlabel('Fraudulence Status (0: Not Fraudulent, 1: Fraudulent)')
    plt.ylabel(col)
    plt.xticks([0, 1], ['Not Fraudulent', 'Fraudulent'])
    plt.tight_layout()
    plt.show()

"""## Categorical and Binary Variables"""

# From the variable info, some columns are described as binary or categorical
# but are stored as integers. We should also include these based on the description.
all_categorical_cols = ["merchant_country", "customer_country", 'city']

for col in all_categorical_cols:
    print(f"\nAnalysis for column: {col}")

    # Calculate frequency of each category
    print("\nFrequency of each category:")
    display(df[col].value_counts())

    # Calculate frequency of each category in relation to fraudulence_status
    print(f"\nFrequency by {col} and fraudulence_status:")
    display(pd.crosstab(df[col], df['Class']))

    # Visualize distribution of categorical variables
    plt.figure(figsize=(10, 6))
    sns.countplot(data=df, x=col, order=df[col].value_counts().index)
    plt.title(f'Distribution of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()

    # Visualize relationship between categorical variable and fraudulence_status
    plt.figure(figsize=(10, 6))
    sns.countplot(data=df, x=col, hue='Class', order=df[col].value_counts().index)
    plt.title(f'Relationship between {col} and Fraudulence Status')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45, ha='right')
    plt.legend(title='Fraudulence Status', labels=['Not Fraudulent', 'Fraudulent'])
    plt.tight_layout()
    plt.show()

"""## Handle Missing Values"""

missing_values = df.isnull().sum()
missing_percentage = (missing_values / len(df)) * 100

missing_info = pd.DataFrame({
    'Missing Values': missing_values,
    'Percentage': missing_percentage
})

missing_info = missing_info[missing_info['Missing Values'] > 0].sort_values(by='Percentage', ascending=False)
display(missing_info)

plt.figure(figsize=(12, 6))
sns.heatmap(df.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Data Heatmap')
plt.show()

df["email"] = df["email"].fillna('Unknown', inplace=True)
df["phone"] = df["phone"].fillna('Unknown', inplace=True)
df["zip_code"] = df["zip_code"].fillna('Unknown', inplace=True)
#customers_df['phone'].fillna('Unknown', inplace=True)

"""## Correlation analysis (Multivariate EDA)

After exploring each variable individually, this section focuses on examining the relationships between numerical features.  
Understanding correlations helps identify redundant variables (those that provide similar information) and detect potential dependencies between features that may influence the model’s performance.  
First, we compute the correlation among all numeric variables to visualize how strongly they are related to each other.

"""

# Compute correlation matrix for all numeric variables
corr_matrix = df.corr(numeric_only=True)

# Display correlation heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(
    corr_matrix,
    cmap='coolwarm',
    center=0,
    linewidths=0.5,
    annot=False
)
plt.title("Correlation Matrix of Numerical Features", fontsize=16)
plt.show()

"""The correlation heatmap shows that the anonymized features (V1–V28) have very low correlations between each other.
This means that each variable provides unique information.
Similarly, Amount, age, and credit_score are also independent from those features, so they add complementary information to the dataset.
Since no strong correlations are observed, we don’t have multicollinearity problems — this is positive for the model training.

Correlation with the target variable `Class`.
"""

# Keep only numeric columns (Class must be numeric 0/1)
num_df = df.select_dtypes(include="number").copy()

# Correlation matrix and correlation vs target
corr = num_df.corr()
target_corr = corr['Class'].drop(labels=['Class'])

# Sort by absolute correlation magnitude (most informative first)
target_corr_sorted = target_corr.reindex(target_corr.abs().sort_values(ascending=False).index)

# Show top/bottom features
display(target_corr_sorted.head(15).to_frame('corr_with_Class'))
display(target_corr_sorted.tail(15).to_frame('corr_with_Class'))

# Plot
plt.figure(figsize=(8, 10))
sns.barplot(
    x=target_corr_sorted.values,
    y=target_corr_sorted.index
)
plt.title("Correlation of Each Numeric Feature with Target (Class)")
plt.xlabel("Correlation coefficient")
plt.ylabel("Feature")
plt.tight_layout()
plt.show()

"""print("Class dtype:", df['Class'].dtype)
print("Class unique values:", df['Class'].unique()[:10])
print("Class value counts:\n", df['Class'].value_counts(dropna=False))
print("Class null %:", df['Class'].isna().mean()*100, "%")

# % de columnas numéricas constantes (varianza 0)
num_df = df.select_dtypes(include="number")
const_cols = [c for c in num_df.columns if num_df[c].nunique(dropna=True) <= 1]
print("Constant numeric columns:", const_cols[:20], f"(total={len(const_cols)})")

## Outliers analysis
"""

outlier_summary = []

for col in numeric_df.columns:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    n_outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
    outlier_summary.append((col, n_outliers, n_outliers/len(df)*100))

outlier_df = pd.DataFrame(outlier_summary, columns=["Feature", "Outliers", "% Outliers"]).sort_values(by="% Outliers", ascending=False)
display(outlier_df.head(15))



"""## Duplicate and uniqueness check"""

print(f"Total duplicated rows: {df.duplicated().sum()}")

id_cols = ['transaction_id', 'customer_id', 'device_id']
for col in id_cols:
    if col in df.columns:
        unique_count = df[col].nunique()
        print(f"{col}: {unique_count} unique values out of {len(df)} ({unique_count/len(df)*100:.2f}% unique)")





"""# PCA
Apply PCA to the numerical features of the dataset, analyze the eigenvalues and eigenvectors, and evaluate the performance of a model trained on the PCA-transformed data.

## Select numerical features

Select the numerical features from the DataFrame for PCA.

**Reasoning**:
Create a new DataFrame containing only numerical columns for PCA.
"""

df_numerical = df.select_dtypes(include=['float64', 'int64']).copy()
display(df_numerical.head())

"""## Scale the data

Standardize the selected numerical features before applying PCA.

**Reasoning**:
Standardize the numerical features using StandardScaler as requested by the subtask.
"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_numeric_data = scaler.fit_transform(df_numerical)
scaled_numeric_df = pd.DataFrame(scaled_numeric_data, columns=df_numerical.columns)
display(scaled_numeric_df.head())

"""## Apply pca

Apply PCA to the scaled data, keeping all components initially to analyze the variance explained by each component.

## Apply pca with selected components

### Subtask:
Apply PCA again, this time specifying the number of components to retain based on the analysis.

**Reasoning**:
Apply PCA again, this time specifying the number of components to retain based on the analysis from the previous step, and then fit the PCA model to the scaled data. From the cumulative explained variance plot, it appears that retaining approximately 10 components explains a significant portion of the variance, and the curve starts to flatten around this point.
"""

from sklearn.decomposition import PCA
pca_final = PCA(n_components=10)
pca_final.fit(scaled_numeric_df)

"""## Analyze eigenvectors

### Subtask:
Examine the eigenvectors to understand the relationship between original features and principal components.

**Reasoning**:
Access the principal components (eigenvectors) from the fitted PCA object and create a DataFrame to show the contribution of original features to each principal component.
"""

principal_components = pca_final.components_
feature_names = scaled_numeric_df.columns
pc_df = pd.DataFrame(principal_components, columns=feature_names, index=[f'PC{i+1}' for i in range(principal_components.shape[0])])
display(pc_df)

plt.figure(figsize=(12, 8))
sns.heatmap(pc_df.T, cmap='viridis', annot=False)
plt.title('Contribution of Original Features to Principal Components')
plt.xlabel('Principal Components')
plt.ylabel('Original Features')
plt.show()

"""# Mutual Information

Calculate mutual information scores between features and the target variable.

**Reasoning**:
Calculate the mutual information between each numerical feature and the target variable 'Class' to understand their dependency.
"""

from sklearn.feature_selection import mutual_info_classif

# Separate features (X) and target (y)
X = df_numerical.drop('Class', axis=1)
y = df_numerical['Class']

# Calculate mutual information scores
mutual_info_scores = mutual_info_classif(X, y, random_state=123)

# Create a Series with feature names and scores
mutual_info_series = pd.Series(mutual_info_scores, index=X.columns)

# Sort the scores in descending order
mutual_info_series_sorted = mutual_info_series.sort_values(ascending=False)

print("Mutual Information Scores (sorted):")
display(mutual_info_series_sorted)

# Visualize the mutual information scores
plt.figure(figsize=(10, 6))
sns.barplot(x=mutual_info_series_sorted.values, y=mutual_info_series_sorted.index)
plt.title('Mutual Information Scores between Features and Class')
plt.xlabel('Mutual Information Score')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

"""# MRmr"""

from sklearn.feature_selection import mutual_info_classif
import numpy as np
import pandas as pd

# Assuming X and y are already defined from previous steps and are pandas DataFrames/Series

# Variable initialization
n_var = X.shape[1]
var_sel = np.empty(0,dtype=int) # subset of selected features
var_cand = np.arange(n_var) # subset of candidate features
feature_names = X.columns # Get original feature names

# Precompute relevances using mutual information for classification
# We need to handle potential discrete features if any exist in X.
# For now, assuming all features in X are continuous.
# If you have discrete features, you would need to provide a 'discrete_features' mask.
relevances = mutual_info_classif(X, y, random_state=123)


# Precompute redundancies (correlation among all input variables)
# Using absolute correlation as a measure of redundancy
redundancies = np.abs(X.corr())
# Convert to numpy array for easier indexing with np.ix_
redundancies_np = redundancies.values


# Select the most relevant feature (with highest mutual information)
sel = np.argmax(relevances)
# Add it to the subset of selected features
var_sel = np.hstack([var_sel,sel])
# Remove it from the subset of candidate features
var_cand = np.delete(var_cand,sel)

MRmr_values= [relevances[sel]]
selected_features = [feature_names[sel]] # Keep track of selected feature names

# Iteratively select variables
for i in range(n_var-1):
    if len(var_cand) == 0:
        break # Stop if no more candidate features

    # Get relevance values of the var_cand variables
    relevances_cand=relevances[var_cand]

    # Compute redundancies with selected features:
    # from the redundancies matrix select the rows of var_sel and the columns of var_cand
    # Ensure indexing is correct for numpy array
    redundancy_sel = redundancies_np[np.ix_(var_sel,var_cand)]
    # Average the redundancy values over the selected features (rows)
    # to get a redundancy value for each candidate variables
    redundancy_cand=np.mean(redundancy_sel,axis=0)

    # Compute MRmr = relevances_cand - redundancy_cand
    MRmr=relevances_cand-redundancy_cand

    # Select the new feature as the one with the maximum MRmr value
    # Handle cases where MRmr might be all negative or zero
    if np.all(MRmr <= 0):
        # If no positive MRmr, select the one with the least negative/closest to zero
        sel = np.argmax(MRmr) # argmax will return the index of the first occurrence of the maximum value
    else:
         sel = np.argmax(MRmr)

    MRmr_values.append(MRmr[sel])
    # Add it to the subset of selected features
    var_sel=np.hstack([var_sel,var_cand[sel]])
    # Keep track of selected feature names
    selected_features.append(feature_names[var_cand[sel]])
    # Remove it from the subset of candidate features
    var_cand= np.delete(var_cand,sel)


# The indices of the relevant features in the original DataFrame X
ind_rel_feat_indices = var_sel
selected_feature_names = selected_features

print("Indices of selected features (in order of selection):", ind_rel_feat_indices)
print("Names of selected features (in order of selection):", selected_feature_names)
print("MRmr values at each selection step:", MRmr_values)

# You can now use selected_feature_names to select columns from your original DataFrame X
# X_mrmr = X[selected_feature_names]

mi_scores_MRmr = pd.Series(MRmr_values, index =X.columns[ind_rel_feat_indices])
mi_scores_MRmr

def plot_utility_scores(scores, title):
    y = scores.sort_values(ascending=True)
    width = np.arange(len(y))
    ticks = list(y.index)
    plt.barh(width, y)
    plt.yticks(width, ticks)
    plt.title(title)

plt.figure(dpi=100, figsize=(10, 10))

plt.rc('ytick', labelsize=8)
plot_utility_scores(mi_scores_MRmr, title="Ranking with MRmr")

from sklearn.metrics.pairwise import rbf_kernel
import numpy as np
import pandas as pd

def center_K(K):
    """Center a kernel matrix K, i.e., removes the data mean in the feature space.

    Args:
        K: kernel matrix
    """
    size_1,size_2 = K.shape;
    D1 = K.sum(axis=0)/size_1
    D2 = K.sum(axis=1)/size_2
    E = D2.sum(axis=0)/size_1

    K_n = K + np.tile(E,[size_1,size_2]) - np.tile(D1,[size_1,1]) - np.tile(D2,[size_2,1]).T
    return K_n

def HSIC_rbf(X_data, Y_data):
    """Compute HSIC value between input and output data using a RBF kernel

    Args:
        X_data: input data (features)
        Y_data: output data (target)
    """
    # Ensure X_data and Y_data are 2D arrays
    if X_data.ndim==1:
        X_data=X_data[:,np.newaxis]
    if Y_data.ndim==1:
        Y_data=Y_data[:,np.newaxis]

    # 2. Compute kernel matrices
    K_x = rbf_kernel(X_data)
    K_y = rbf_kernel(Y_data)

    # 3. Center kernel matrices
    K_xc = center_K(K_x)
    K_yc = center_K(K_y)

    # 4. Compute HSIC value
    HSIC= float(np.multiply(K_xc,K_yc).sum().sum())/(K_x.shape[0]**2) ;
    #HSIC= float(np.trace(np.dot(K_xc,K_yc)))/(K_x.shape[0]**2)
    return HSIC




# Compute HSIC relevances
importances=np.zeros(X.shape[1])
for i in range(X.shape[1]):
    # We need to pass a 2D array for X_data and Y_data to rbf_kernel
    importances[i]=HSIC_rbf(X.iloc[:,i].values.reshape(-1, 1), y.values.reshape(-1, 1))

# Obtain the positions of the sorted features (the most relevant first)
ind_rel_feat = np.argsort(importances)[::-1]

HSIC_scores = pd.Series(importances[ind_rel_feat], index =X.columns[ind_rel_feat])
display(HSIC_scores)

plt.figure(figsize=(10, 6))
sns.barplot(x=HSIC_scores.values, y=HSIC_scores.index)
plt.title('HSIC Scores between Features and Class (RBF Kernel)')
plt.xlabel('HSIC Score')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()





# Separate features (X) and target (y)
X = df_numerical.drop('Class', axis=1)
y = df_numerical['Class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define the models
models = {
    'XGBoost': xgb.XGBClassifier(objective='binary:logistic', n_estimators=100, learning_rate=0.1, random_state=42, use_label_encoder=False, eval_metric='logloss'),
    'LightGBM': lgbm.LGBMClassifier(objective='binary', n_estimators=100, learning_rate=0.1, random_state=42),
    'CatBoost': CatBoostClassifier(iterations=100, learning_rate=0.1, random_state=42, verbose=0)
}

results = {}
feature_importance = {}

# Train and evaluate each model
for name, model in models.items():
    print(f"Training {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results[name] = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall': recall,
        'F1 Score': f1
    }
    print(f"{name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}")

    # Get feature importance
    if hasattr(model, 'feature_importances_'):
        feature_importance[name] = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)
    elif hasattr(model, 'coef_'):
        feature_importance[name] = pd.Series(model.coef_[0], index=X.columns).sort_values(ascending=False)


# Display the results
results_df = pd.DataFrame(results).T
display(results_df)

# Display feature importance
for name, importance in feature_importance.items():
    print(f"\nFeature Importance for {name}:")
    display(importance)
    plt.figure(figsize=(10, 6))
    sns.barplot(x=importance.values, y=importance.index)
    plt.title(f'Feature Importance for {name}')
    plt.xlabel('Importance')
    plt.ylabel('Feature')
    plt.tight_layout()
    plt.show()

    # TODO copiar SMOTE notebook